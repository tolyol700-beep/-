import aiohttp
import asyncio
from bs4 import BeautifulSoup
from config import SOURCES
import logging
import re

class ContentAggregator:
    async def fetch_rss(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS-–ª–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é BeautifulSoup"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        posts = []
                        
                        # –ü–∞—Ä—Å–∏–º RSS/Atom —Ñ–∏–¥—ã
                        items = soup.find_all('item')[:5] or soup.find_all('entry')[:5]
                        
                        for item in items:
                            try:
                                title_elem = item.find('title')
                                link_elem = item.find('link')
                                description_elem = item.find('description') or item.find('summary')
                                pub_date_elem = item.find('pubdate') or item.find('published')
                                
                                if title_elem:
                                    title = title_elem.get_text().strip()
                                    
                                    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                                    if link_elem:
                                        if link_elem.get('href'):
                                            link = link_elem.get('href')
                                        else:
                                            link = link_elem.get_text().strip()
                                    else:
                                        link = url
                                    
                                    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                                    if description_elem:
                                        description = description_elem.get_text().strip()[:300] + "..."
                                    else:
                                        description = title
                                    
                                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                                    pub_date = ""
                                    if pub_date_elem:
                                        pub_date = pub_date_elem.get_text().strip()
                                    
                                    post = {
                                        'title': title,
                                        'link': link,
                                        'summary': description,
                                        'published': pub_date,
                                        'source': url
                                    }
                                    posts.append(post)
                            except Exception as e:
                                logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ RSS: {e}")
                                continue
                        
                        return posts
                    else:
                        logging.error(f"–û—à–∏–±–∫–∞ HTTP {response.status} –¥–ª—è {url}")
                        return []
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {url}: {e}")
            return []
            
    async def fetch_web_content(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        posts = []
                        
                        # –£–±–∏—Ä–∞–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                        for script in soup(["script", "style"]):
                            script.decompose()
                        
                        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        title = soup.find('title')
                        if title:
                            title_text = title.get_text().strip()
                        else:
                            title_text = "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                        
                        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                        article = (soup.find('article') or 
                                  soup.find('div', class_=re.compile('content|article|post')) or
                                  soup.find('main') or
                                  soup)
                        
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
                        text_content = article.get_text().strip()
                        summary = ' '.join(text_content.split()[:30]) + "..."
                        
                        post = {
                            'title': title_text,
                            'link': url,
                            'summary': summary,
                            'source': url
                        }
                        posts.append(post)
                        return posts
                    else:
                        return []
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return []
            
    async def fetch_content(self, content_type):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        sources = SOURCES.get(content_type, [])
        all_posts = []
        
        for source in sources:
            try:
                if source.endswith('.rss') or source.endswith('.xml') or 'rss' in source:
                    posts = await self.fetch_rss(source)
                else:
                    posts = await self.fetch_web_content(source)
                    
                all_posts.extend(posts)
                logging.info(f"üì∞ –ò–∑ {source} –ø–æ–ª—É—á–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤")
                
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source}: {e}")
                continue
                
        return all_posts
