import aiohttp
import asyncio
from bs4 import BeautifulSoup
from config import SOURCES
import logging
import xml.etree.ElementTree as ET
from urllib.parse import urlparse

class ContentAggregator:
    async def fetch_rss(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS-–ª–µ–Ω—Ç –±–µ–∑ feedparser"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        posts = []
                        try:
                            # –ü–∞—Ä—Å–∏–º XML –Ω–∞–ø—Ä—è–º—É—é
                            root = ET.fromstring(content)
                            
                            # –ò—â–µ–º items (RSS 2.0) –∏–ª–∏ entries (Atom)
                            items = root.findall('.//item') or root.findall('.//entry')
                            
                            for item in items[:5]:  # –ë–µ—Ä–µ–º 5 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
                                title_elem = item.find('title') or item.find('{http://www.w3.org/2005/Atom}title')
                                link_elem = item.find('link') or item.find('{http://www.w3.org/2005/Atom}link')
                                description_elem = item.find('description') or item.find('{http://www.w3.org/2005/Atom}summary')
                                pub_date_elem = item.find('pubDate') or item.find('{http://www.w3.org/2005/Atom}published')
                                
                                if title_elem is not None:
                                    post = {
                                        'title': title_elem.text or '',
                                        'link': link_elem.text if link_elem is not None else (link_elem.get('href') if link_elem is not None else url),
                                        'summary': description_elem.text[:300] + "..." if description_elem is not None and description_elem.text else title_elem.text or '',
                                        'published': pub_date_elem.text if pub_date_elem is not None else '',
                                        'source': url
                                    }
                                    posts.append(post)
                        except ET.ParseError:
                            # –ï—Å–ª–∏ XML –Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup
                            soup = BeautifulSoup(content, 'xml')
                            items = soup.find_all('item')[:5]
                            
                            for item in items:
                                title = item.find('title')
                                link = item.find('link')
                                description = item.find('description')
                                pub_date = item.find('pubDate')
                                
                                if title:
                                    post = {
                                        'title': title.get_text(),
                                        'link': link.get_text() if link else url,
                                        'summary': description.get_text()[:300] + "..." if description else title.get_text(),
                                        'published': pub_date.get_text() if pub_date else '',
                                        'source': url
                                    }
                                    posts.append(post)
                        
                        return posts
                    else:
                        logging.error(f"–û—à–∏–±–∫–∞ HTTP {response.status} –¥–ª—è {url}")
                        return []
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {url}: {e}")
            return []
            
    async def fetch_web_content(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # –ü–∞—Ä—Å–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ
                        posts = []
                        title = soup.find('title')
                        meta_description = soup.find('meta', attrs={'name': 'description'})
                        
                        if title:
                            description = ""
                            if meta_description:
                                description = meta_description.get('content', '')[:200]
                            
                            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                            article = soup.find('article') or soup.find('div', class_=lambda x: x and ('content' in x or 'article' in x))
                            if article:
                                description = article.get_text()[:200].strip()
                            
                            post = {
                                'title': title.get_text().strip(),
                                'link': url,
                                'summary': description or "–ß–∏—Ç–∞–π—Ç–µ –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é –ø–æ —Å—Å—ã–ª–∫–µ",
                                'source': url
                            }
                            posts.append(post)
                        return posts
                    else:
                        return []
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return []
            
    async def fetch_content(self, content_type):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        sources = SOURCES.get(content_type, [])
        all_posts = []
        
        tasks = []
        for source in sources:
            if source.endswith('.rss') or source.endswith('.xml'):
                task = self.fetch_rss(source)
            else:
                task = self.fetch_web_content(source)
            tasks.append(task)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {result}")
                continue
            all_posts.extend(result)
                
        logging.info(f"üì• –î–ª—è {content_type} —Å–æ–±—Ä–∞–Ω–æ {len(all_posts)} –ø–æ—Å—Ç–æ–≤")
        return all_posts
