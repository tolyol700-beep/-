import aiohttp
import asyncio
from bs4 import BeautifulSoup
from config import SOURCES
import logging
import re
import ssl

class ContentAggregator:
    def __init__(self):
        # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π SSL –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º —Å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE

    async def fetch_rss(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS-–ª–µ–Ω—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            connector = aiohttp.TCPConnector(ssl=self.ssl_context)
            timeout = aiohttp.ClientTimeout(total=15)
            
            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                        for encoding in ['utf-8', 'windows-1251', 'cp1251']:
                            try:
                                soup = BeautifulSoup(content, 'xml', from_encoding=encoding)
                                break
                            except:
                                continue
                        else:
                            soup = BeautifulSoup(content, 'xml')
                        
                        posts = []
                        
                        # –ò—â–µ–º items –≤ RSS –∏–ª–∏ entries –≤ Atom
                        items = soup.find_all('item') or soup.find_all('entry')
                        
                        for item in items[:5]:  # –ë–µ—Ä–µ–º 5 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
                            try:
                                title_elem = item.find('title')
                                if not title_elem:
                                    continue
                                
                                title = title_elem.get_text().strip()
                                
                                # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                                link_elem = item.find('link')
                                if link_elem:
                                    link = link_elem.get('href') or link_elem.get_text()
                                else:
                                    guid_elem = item.find('guid')
                                    link = guid_elem.get_text() if guid_elem else url
                                
                                # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                                description_elem = item.find('description') or item.find('summary') or item.find('content')
                                description = ""
                                if description_elem:
                                    description = description_elem.get_text().strip()[:300]
                                
                                # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                                if not description:
                                    description = title
                                
                                # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É
                                pub_date_elem = item.find('pubdate') or item.find('published')
                                pub_date = pub_date_elem.get_text() if pub_date_elem else ""
                                
                                post = {
                                    'title': title,
                                    'link': link,
                                    'summary': description + "..." if len(description) > 100 else description,
                                    'published': pub_date,
                                    'source': url
                                }
                                posts.append(post)
                                
                            except Exception as e:
                                logging.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ {url}: {e}")
                                continue
                        
                        logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –∏–∑ {url}")
                        return posts
                    else:
                        logging.warning(f"‚ö†Ô∏è HTTP {response.status} –¥–ª—è {url}")
                        return []
                        
        except asyncio.TimeoutError:
            logging.error(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url}")
            return []
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {url}: {str(e)}")
            return []
            
    async def fetch_web_content(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            connector = aiohttp.TCPConnector(ssl=self.ssl_context)
            timeout = aiohttp.ClientTimeout(total=15)
            
            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.text()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                        for element in soup(["script", "style", "nav", "header", "footer"]):
                            element.decompose()
                        
                        title = soup.find('title')
                        title_text = title.get_text().strip() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                        
                        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                        article = (soup.find('article') or 
                                  soup.find('div', class_=re.compile('content|article|post|main')) or
                                  soup.find('main') or
                                  soup.find('body'))
                        
                        if article:
                            text = article.get_text()
                            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                            text = ' '.join(text.split())
                            summary = text[:200] + "..." if len(text) > 200 else text
                        else:
                            summary = "–ß–∏—Ç–∞–π—Ç–µ –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é –ø–æ —Å—Å—ã–ª–∫–µ"
                        
                        post = {
                            'title': title_text,
                            'link': url,
                            'summary': summary,
                            'source': url
                        }
                        return [post]
                    else:
                        logging.warning(f"‚ö†Ô∏è HTTP {response.status} –¥–ª—è –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}")
                        return []
                        
        except asyncio.TimeoutError:
            logging.error(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")
            return []
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {str(e)}")
            return []
            
    async def fetch_content(self, content_type):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        sources = SOURCES.get(content_type, [])
        all_posts = []
        
        logging.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ç–∏–ø–∞ '{content_type}' –∏–∑ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        for source in sources:
            try:
                if any(ext in source for ext in ['.rss', '.xml', 'rss/']):
                    posts = await self.fetch_rss(source)
                else:
                    posts = await self.fetch_web_content(source)
                    
                if posts:
                    all_posts.extend(posts)
                    logging.info(f"üì∞ –ò–∑ {source} –ø–æ–ª—É—á–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤")
                else:
                    logging.warning(f"üì≠ –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã –∏–∑ {source}")
                    
            except Exception as e:
                logging.error(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source}: {e}")
                continue
                
        logging.info(f"üì• –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(all_posts)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è {content_type}")
        return all_posts
